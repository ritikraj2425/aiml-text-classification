{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé® Gradio Web Interface for Unified Hate Speech Classifier\n",
                "\n",
                "This notebook provides a beautiful web interface for the unified classifier.\n",
                "\n",
                "**Features:**\n",
                "- Automatic language detection (Hindi/English)\n",
                "- Real-time classification\n",
                "- Probability distribution visualization\n",
                "- Example texts for quick testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "# Install required libraries\n",
                "!pip install langdetect transformers torch gradio --quiet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import os\n",
                "import gradio as gr\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "from langdetect import detect, DetectorFactory\n",
                "from langdetect.lang_detect_exception import LangDetectException\n",
                "import warnings\n",
                "\n",
                "# Set seed for consistent language detection results\n",
                "DetectorFactory.seed = 0\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(f\"‚úÖ Libraries imported! Gradio version: {gr.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ LanguageDetector defined!\n"
                    ]
                }
            ],
            "source": [
                "class LanguageDetector:\n",
                "    \"\"\"Language detector that identifies Hindi and English text.\"\"\"\n",
                "    \n",
                "    DEVANAGARI_RANGE = (0x0900, 0x097F)\n",
                "    ASCII_RANGE = (0x0041, 0x007A)\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.supported_languages = {'hi': 'hindi', 'en': 'english'}\n",
                "    \n",
                "    def _get_script_ratio(self, text):\n",
                "        devanagari_count = latin_count = total_chars = 0\n",
                "        for char in text:\n",
                "            code = ord(char)\n",
                "            if char.isalpha():\n",
                "                total_chars += 1\n",
                "                if self.DEVANAGARI_RANGE[0] <= code <= self.DEVANAGARI_RANGE[1]:\n",
                "                    devanagari_count += 1\n",
                "                elif self.ASCII_RANGE[0] <= code <= self.ASCII_RANGE[1]:\n",
                "                    latin_count += 1\n",
                "        if total_chars == 0:\n",
                "            return 0, 0\n",
                "        return devanagari_count / total_chars, latin_count / total_chars\n",
                "    \n",
                "    def detect(self, text):\n",
                "        text = str(text).strip()\n",
                "        if not text:\n",
                "            return 'unsupported', 'not_supported', 0.0\n",
                "        \n",
                "        devanagari_ratio, latin_ratio = self._get_script_ratio(text)\n",
                "        \n",
                "        if devanagari_ratio > 0.1 and latin_ratio > 0.1:\n",
                "            return 'unsupported', 'not_supported (hinglish/mixed)', 0.0\n",
                "        if devanagari_ratio > 0.7:\n",
                "            return 'hi', 'hindi', devanagari_ratio\n",
                "        if latin_ratio > 0.7:\n",
                "            try:\n",
                "                detected_lang = detect(text)\n",
                "                if detected_lang == 'en':\n",
                "                    return 'en', 'english', latin_ratio\n",
                "                return 'unsupported', f'not_supported ({detected_lang})', 0.0\n",
                "            except LangDetectException:\n",
                "                return 'unsupported', 'not_supported', 0.0\n",
                "        try:\n",
                "            detected_lang = detect(text)\n",
                "            if detected_lang in self.supported_languages:\n",
                "                return detected_lang, self.supported_languages[detected_lang], 0.5\n",
                "            return 'unsupported', f'not_supported ({detected_lang})', 0.0\n",
                "        except LangDetectException:\n",
                "            return 'unsupported', 'not_supported', 0.0\n",
                "\n",
                "print(\"‚úÖ LanguageDetector defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ UnifiedHateSpeechClassifier defined!\n"
                    ]
                }
            ],
            "source": [
                "class UnifiedHateSpeechClassifier:\n",
                "    \"\"\"Unified classifier with automatic language detection.\"\"\"\n",
                "    \n",
                "    HUGGINGFACE_ENGLISH_MODEL = \"Hate-speech-CNERG/bert-base-uncased-hatexplain\"\n",
                "    \n",
                "    def __init__(self, english_model_path=\"./model\", hindi_model_path=\"./hindi_text_classifier\"):\n",
                "        print(\"üöÄ Initializing Unified Classifier...\")\n",
                "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "        print(f\"üì± Device: {self.device}\")\n",
                "        \n",
                "        self.language_detector = LanguageDetector()\n",
                "        self._load_english_model(english_model_path)\n",
                "        self._load_hindi_model(hindi_model_path)\n",
                "        print(\"‚úÖ Classifier Ready!\")\n",
                "    \n",
                "    def _load_english_model(self, path):\n",
                "        print(\"üìö Loading English model...\")\n",
                "        local_weights = os.path.exists(os.path.join(path, \"pytorch_model.bin\")) or \\\n",
                "                        os.path.exists(os.path.join(path, \"model.safetensors\"))\n",
                "        \n",
                "        if local_weights:\n",
                "            self.english_tokenizer = AutoTokenizer.from_pretrained(path)\n",
                "            self.english_model = AutoModelForSequenceClassification.from_pretrained(path)\n",
                "        else:\n",
                "            print(f\"   üì• Downloading from HuggingFace...\")\n",
                "            self.english_tokenizer = AutoTokenizer.from_pretrained(self.HUGGINGFACE_ENGLISH_MODEL)\n",
                "            self.english_model = AutoModelForSequenceClassification.from_pretrained(self.HUGGINGFACE_ENGLISH_MODEL)\n",
                "        \n",
                "        self.english_labels = self.english_model.config.id2label\n",
                "        self.english_model.to(self.device).eval()\n",
                "    \n",
                "    def _load_hindi_model(self, path):\n",
                "        print(\"üìö Loading Hindi model...\")\n",
                "        self.hindi_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
                "        self.hindi_model = AutoModelForSequenceClassification.from_pretrained(path)\n",
                "        self.hindi_labels = ['hate', 'normal', 'offensive']\n",
                "        self.hindi_model.to(self.device).eval()\n",
                "    \n",
                "    def _predict(self, text, tokenizer, model, labels):\n",
                "        inputs = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
                "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
                "        with torch.no_grad():\n",
                "            outputs = model(**inputs)\n",
                "        probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
                "        pred_idx = torch.argmax(probs).item()\n",
                "        prob_dict = {labels[i]: round(probs[i].item(), 4) for i in range(len(probs))}\n",
                "        return labels[pred_idx], probs[pred_idx].item(), prob_dict\n",
                "    \n",
                "    def classify(self, text):\n",
                "        text = str(text).strip()\n",
                "        if not text:\n",
                "            return {'text': text, 'detected_language': 'empty', 'prediction': 'error', \n",
                "                    'confidence': 0.0, 'probabilities': {}, 'model_used': None}\n",
                "        \n",
                "        lang_code, lang_name, _ = self.language_detector.detect(text)\n",
                "        \n",
                "        if lang_code == 'en':\n",
                "            pred, conf, probs = self._predict(text, self.english_tokenizer, self.english_model, self.english_labels)\n",
                "            model_used = 'English (BERT)'\n",
                "        elif lang_code == 'hi':\n",
                "            pred, conf, probs = self._predict(text, self.hindi_tokenizer, self.hindi_model, self.hindi_labels)\n",
                "            model_used = 'Hindi (XLM-RoBERTa)'\n",
                "        else:\n",
                "            return {'text': text, 'detected_language': lang_name, 'prediction': 'NOT SUPPORTED',\n",
                "                    'confidence': 0.0, 'probabilities': {}, 'model_used': None,\n",
                "                    'message': f'Language \"{lang_name}\" is not supported.'}\n",
                "        \n",
                "        return {'text': text, 'detected_language': lang_name, 'prediction': pred,\n",
                "                'confidence': round(conf, 4), 'probabilities': probs, 'model_used': model_used}\n",
                "\n",
                "print(\"‚úÖ UnifiedHateSpeechClassifier defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Initializing Unified Classifier...\n",
                        "üì± Device: cpu\n",
                        "üìö Loading English model...\n",
                        "   üì• Downloading from HuggingFace...\n",
                        "üìö Loading Hindi model...\n",
                        "‚úÖ Classifier Ready!\n"
                    ]
                }
            ],
            "source": [
                "# Initialize the classifier\n",
                "classifier = UnifiedHateSpeechClassifier(\n",
                "    english_model_path=\"./model\",\n",
                "    hindi_model_path=\"./hindi_text_classifier\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Gradio interface ready! Run next cell to launch.\n"
                    ]
                }
            ],
            "source": [
                "def classify_text(text):\n",
                "    \"\"\"Classify text and return formatted results for Gradio.\"\"\"\n",
                "    if not text or not text.strip():\n",
                "        return \"‚ö†Ô∏è Please enter text\", \"‚ùì Unknown\", \"N/A\", {}\n",
                "    \n",
                "    result = classifier.classify(text)\n",
                "    lang = result['detected_language'].upper()\n",
                "    \n",
                "    if result['prediction'] == 'NOT SUPPORTED':\n",
                "        return f\"üåê {lang}\", \"‚ùå NOT SUPPORTED\", result.get('message', 'Not supported'), {}\n",
                "    \n",
                "    pred = result['prediction'].upper()\n",
                "    emoji = {'HATE': 'üî¥', 'NORMAL': 'üü¢', 'OFFENSIVE': 'üü°', 'HATE SPEECH': 'üî¥'}.get(pred, '‚ö™')\n",
                "    model_info = f\"ü§ñ {result['model_used']} | üìà {result['confidence']:.1%}\"\n",
                "    \n",
                "    return f\"üåê {lang}\", f\"{emoji} {pred}\", model_info, result['probabilities']\n",
                "\n",
                "# Create Gradio Interface (compatible with Gradio 6.x)\n",
                "with gr.Blocks(title=\"Unified Hate Speech Classifier\") as demo:\n",
                "    \n",
                "    gr.Markdown(\"\"\"\n",
                "    # üõ°Ô∏è Unified Hate Speech Classifier\n",
                "    \n",
                "    **Automatically detects language and classifies text as Hate Speech, Offensive, or Normal.**\n",
                "    \n",
                "    ‚úÖ Supports: **English** | **Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä)**  \n",
                "    ‚ùå Not Supported: Hinglish, French, German, Japanese, etc.\n",
                "    \"\"\")\n",
                "    \n",
                "    with gr.Row():\n",
                "        with gr.Column(scale=2):\n",
                "            text_input = gr.Textbox(\n",
                "                label=\"Enter Text to Classify\",\n",
                "                placeholder=\"Type in English or Hindi...\\n\\nExample: 'Hello world!' or '‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ'\",\n",
                "                lines=4\n",
                "            )\n",
                "            classify_btn = gr.Button(\"üîç Classify\", variant=\"primary\")\n",
                "        \n",
                "        with gr.Column(scale=1):\n",
                "            lang_out = gr.Textbox(label=\"Language\", interactive=False)\n",
                "            pred_out = gr.Textbox(label=\"Prediction\", interactive=False)\n",
                "            model_out = gr.Textbox(label=\"Model & Confidence\", interactive=False)\n",
                "    \n",
                "    prob_out = gr.Label(label=\"Probability Distribution\", num_top_classes=3)\n",
                "    \n",
                "    gr.Markdown(\"### üìù Examples:\")\n",
                "    gr.Examples(\n",
                "        examples=[\n",
                "            [\"I love spending time with my family\"],\n",
                "            [\"You are such a stupid idiot\"],\n",
                "            [\"All immigrants should be deported\"],\n",
                "            [\"‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§®‡§∞‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Æ‡•ã‡§¶‡•Ä ‡§π‡•à\"],\n",
                "            [\"‡§Ø‡•á ‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§ï‡•Ä ‡§î‡§≤‡§æ‡§¶ ‡§π‡•à ‡§∏‡§¨\"],\n",
                "            [\"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§∏‡•Å‡§π‡§æ‡§µ‡§®‡§æ ‡§π‡•à\"],\n",
                "            [\"Ye bahut bakwaas hai ‡§Ø‡§æ‡§∞\"],\n",
                "            [\"Je t'aime beaucoup\"],\n",
                "        ],\n",
                "        inputs=text_input\n",
                "    )\n",
                "    \n",
                "    classify_btn.click(fn=classify_text, inputs=text_input, outputs=[lang_out, pred_out, model_out, prob_out])\n",
                "    text_input.submit(fn=classify_text, inputs=text_input, outputs=[lang_out, pred_out, model_out, prob_out])\n",
                "    \n",
                "    gr.Markdown(\"---\\n**Legend:** üü¢ Normal | üü° Offensive | üî¥ Hate Speech\")\n",
                "\n",
                "print(\"‚úÖ Gradio interface ready! Run next cell to launch.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "* Running on local URL:  http://127.0.0.1:7861\n",
                        "* To create a public link, set `share=True` in `launch()`.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": []
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
                    ]
                }
            ],
            "source": [
                "# üöÄ Launch the Gradio interface\n",
                "demo.launch()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "data-mining",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
