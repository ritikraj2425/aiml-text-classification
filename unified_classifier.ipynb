{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåê Unified Hate Speech Classifier with Language Detection\n",
                "\n",
                "This notebook provides a unified interface that:\n",
                "1. **Detects** if input text is Hindi or English using a pretrained language detection model\n",
                "2. **Routes** Hindi text ‚Üí Hindi model (XLM-RoBERTa)\n",
                "3. **Routes** English text ‚Üí English model (BERT)\n",
                "4. **Rejects** other languages (including Hinglish, mixed scripts, etc.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required libraries\n",
                "!pip install langdetect transformers torch --quiet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import os\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, BertConfig\n",
                "from langdetect import detect, DetectorFactory\n",
                "from langdetect.lang_detect_exception import LangDetectException\n",
                "import warnings\n",
                "\n",
                "DetectorFactory.seed = 0\n",
                "\n",
                "# Suppress warnings for cleaner output\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ LanguageDetector class defined!\n"
                    ]
                }
            ],
            "source": [
                "class LanguageDetector:\n",
                "    \"\"\"\n",
                "    Language detector that identifies Hindi and English text.\n",
                "    Uses the langdetect library with additional validation for script detection.\n",
                "    \"\"\"\n",
                "    \n",
                "    # Unicode ranges for different scripts\n",
                "    DEVANAGARI_RANGE = (0x0900, 0x097F)  # Hindi script\n",
                "    ASCII_RANGE = (0x0041, 0x007A)        # Basic Latin letters\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.supported_languages = {'hi': 'hindi', 'en': 'english'}\n",
                "    \n",
                "    def _get_script_ratio(self, text):\n",
                "        \"\"\"Calculate the ratio of different scripts in the text.\"\"\"\n",
                "        devanagari_count = 0\n",
                "        latin_count = 0\n",
                "        total_chars = 0\n",
                "        \n",
                "        for char in text:\n",
                "            code = ord(char)\n",
                "            if char.isalpha():\n",
                "                total_chars += 1\n",
                "                if self.DEVANAGARI_RANGE[0] <= code <= self.DEVANAGARI_RANGE[1]:\n",
                "                    devanagari_count += 1\n",
                "                elif self.ASCII_RANGE[0] <= code <= self.ASCII_RANGE[1]:\n",
                "                    latin_count += 1\n",
                "        \n",
                "        if total_chars == 0:\n",
                "            return 0, 0\n",
                "        \n",
                "        return devanagari_count / total_chars, latin_count / total_chars\n",
                "    \n",
                "    def detect(self, text):\n",
                "        \"\"\"\n",
                "        Detect the language of the input text.\n",
                "        \n",
                "        Returns:\n",
                "            tuple: (language_code, language_name, confidence)\n",
                "        \"\"\"\n",
                "        text = str(text).strip()\n",
                "        \n",
                "        if not text:\n",
                "            return 'unsupported', 'not_supported', 0.0\n",
                "        \n",
                "        # First, check script composition\n",
                "        devanagari_ratio, latin_ratio = self._get_script_ratio(text)\n",
                "        \n",
                "        # If text is predominantly Hinglish (mixed scripts), reject it\n",
                "        if devanagari_ratio > 0.1 and latin_ratio > 0.1:\n",
                "            return 'unsupported', 'not_supported (hinglish/mixed)', 0.0\n",
                "        \n",
                "        # Pure Devanagari script ‚Üí Hindi\n",
                "        if devanagari_ratio > 0.7:\n",
                "            return 'hi', 'hindi', devanagari_ratio\n",
                "        \n",
                "        # Pure Latin script ‚Üí Use langdetect for further classification\n",
                "        if latin_ratio > 0.7:\n",
                "            try:\n",
                "                detected_lang = detect(text)\n",
                "                if detected_lang == 'en':\n",
                "                    return 'en', 'english', latin_ratio\n",
                "                else:\n",
                "                    return 'unsupported', f'not_supported ({detected_lang})', 0.0\n",
                "            except LangDetectException:\n",
                "                return 'unsupported', 'not_supported', 0.0\n",
                "        \n",
                "        # Fallback: try langdetect\n",
                "        try:\n",
                "            detected_lang = detect(text)\n",
                "            if detected_lang in self.supported_languages:\n",
                "                return detected_lang, self.supported_languages[detected_lang], 0.5\n",
                "            else:\n",
                "                return 'unsupported', f'not_supported ({detected_lang})', 0.0\n",
                "        except LangDetectException:\n",
                "            return 'unsupported', 'not_supported', 0.0\n",
                "\n",
                "print(\"‚úÖ LanguageDetector class defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ UnifiedHateSpeechClassifier class defined!\n"
                    ]
                }
            ],
            "source": [
                "class UnifiedHateSpeechClassifier:\n",
                "    \"\"\"\n",
                "    Unified Hate Speech Classifier that automatically detects language\n",
                "    and routes to the appropriate model (Hindi or English).\n",
                "    \"\"\"\n",
                "    \n",
                "    # Fallback HuggingFace model for English hate speech detection\n",
                "    # This is used when local model weights are not available\n",
                "    HUGGINGFACE_ENGLISH_MODEL = \"Hate-speech-CNERG/bert-base-uncased-hatexplain\"\n",
                "    \n",
                "    def __init__(self, \n",
                "                 english_model_path=\"./model\",\n",
                "                 hindi_model_path=\"./hindi_text_classifier\"):\n",
                "        \"\"\"\n",
                "        Initialize the unified classifier.\n",
                "        \"\"\"\n",
                "        print(\"üöÄ Initializing Unified Hate Speech Classifier...\")\n",
                "        print(\"=\" * 60)\n",
                "        \n",
                "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "        print(f\"üì± Device: {self.device}\")\n",
                "        \n",
                "        # Initialize language detector\n",
                "        self.language_detector = LanguageDetector()\n",
                "        print(\"‚úÖ Language Detector initialized\")\n",
                "        \n",
                "        # Load English model\n",
                "        self._load_english_model(english_model_path)\n",
                "        \n",
                "        # Load Hindi model\n",
                "        print(\"\\nüìö Loading Hindi model (XLM-RoBERTa)...\")\n",
                "        self.hindi_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
                "        self.hindi_model = AutoModelForSequenceClassification.from_pretrained(hindi_model_path)\n",
                "        self.hindi_model.to(self.device)\n",
                "        self.hindi_model.eval()\n",
                "        self.hindi_labels = ['hate', 'normal', 'offensive']\n",
                "        print(f\"   Classes: {self.hindi_labels}\")\n",
                "        \n",
                "        print(\"\\n\" + \"=\" * 60)\n",
                "        print(\"‚úÖ Unified Classifier Ready!\")\n",
                "        print(\"   Supported Languages: Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä), English\")\n",
                "        print(\"=\" * 60 + \"\\n\")\n",
                "    \n",
                "    def _load_english_model(self, english_model_path):\n",
                "        \"\"\"Load English model, with fallback to HuggingFace if local weights missing.\"\"\"\n",
                "        print(\"\\nüìö Loading English model (BERT)...\")\n",
                "        \n",
                "        # Check if local model has weights\n",
                "        local_weights_exist = (\n",
                "            os.path.exists(os.path.join(english_model_path, \"pytorch_model.bin\")) or\n",
                "            os.path.exists(os.path.join(english_model_path, \"model.safetensors\"))\n",
                "        )\n",
                "        \n",
                "        if local_weights_exist:\n",
                "            # Load from local path\n",
                "            print(\"   Loading from local path...\")\n",
                "            self.english_tokenizer = AutoTokenizer.from_pretrained(english_model_path)\n",
                "            self.english_model = AutoModelForSequenceClassification.from_pretrained(english_model_path)\n",
                "            self.english_labels = self.english_model.config.id2label\n",
                "        else:\n",
                "            # Fallback to HuggingFace model\n",
                "            print(f\"   ‚ö†Ô∏è  Local weights not found at '{english_model_path}'\")\n",
                "            print(f\"   üì• Downloading from HuggingFace: {self.HUGGINGFACE_ENGLISH_MODEL}\")\n",
                "            \n",
                "            self.english_tokenizer = AutoTokenizer.from_pretrained(self.HUGGINGFACE_ENGLISH_MODEL)\n",
                "            self.english_model = AutoModelForSequenceClassification.from_pretrained(self.HUGGINGFACE_ENGLISH_MODEL)\n",
                "            self.english_labels = self.english_model.config.id2label\n",
                "        \n",
                "        self.english_model.to(self.device)\n",
                "        self.english_model.eval()\n",
                "        print(f\"   Classes: {list(self.english_labels.values())}\")\n",
                "    \n",
                "    def _predict_english(self, text):\n",
                "        \"\"\"Classify English text using BERT model.\"\"\"\n",
                "        inputs = self.english_tokenizer(\n",
                "            text,\n",
                "            truncation=True,\n",
                "            padding=True,\n",
                "            max_length=128,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = self.english_model(**inputs)\n",
                "        \n",
                "        probabilities = torch.softmax(outputs.logits, dim=-1)\n",
                "        prediction_idx = torch.argmax(probabilities, dim=-1).item()\n",
                "        confidence = probabilities[0][prediction_idx].item()\n",
                "        \n",
                "        prob_dict = {\n",
                "            self.english_labels[idx]: round(prob.item(), 4)\n",
                "            for idx, prob in enumerate(probabilities[0])\n",
                "        }\n",
                "        \n",
                "        return {\n",
                "            'prediction': self.english_labels[prediction_idx],\n",
                "            'confidence': round(confidence, 4),\n",
                "            'probabilities': prob_dict\n",
                "        }\n",
                "    \n",
                "    def _predict_hindi(self, text):\n",
                "        \"\"\"Classify Hindi text using XLM-RoBERTa model.\"\"\"\n",
                "        inputs = self.hindi_tokenizer(\n",
                "            text,\n",
                "            truncation=True,\n",
                "            padding=True,\n",
                "            max_length=128,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = self.hindi_model(**inputs)\n",
                "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
                "            prediction_idx = torch.argmax(probabilities, dim=1).item()\n",
                "            confidence = probabilities[0][prediction_idx].item()\n",
                "        \n",
                "        prob_dict = {\n",
                "            self.hindi_labels[idx]: round(prob.item(), 4)\n",
                "            for idx, prob in enumerate(probabilities[0])\n",
                "        }\n",
                "        \n",
                "        return {\n",
                "            'prediction': self.hindi_labels[prediction_idx],\n",
                "            'confidence': round(confidence, 4),\n",
                "            'probabilities': prob_dict\n",
                "        }\n",
                "    \n",
                "    def classify(self, text):\n",
                "        \"\"\"\n",
                "        Classify input text for hate speech.\n",
                "        Automatically detects language and routes to appropriate model.\n",
                "        \"\"\"\n",
                "        text = str(text).strip()\n",
                "        \n",
                "        if not text:\n",
                "            return {\n",
                "                'text': text,\n",
                "                'detected_language': 'empty',\n",
                "                'prediction': 'error',\n",
                "                'confidence': 0.0,\n",
                "                'probabilities': {},\n",
                "                'model_used': None,\n",
                "                'error': 'Empty text provided'\n",
                "            }\n",
                "        \n",
                "        # Detect language\n",
                "        lang_code, lang_name, lang_confidence = self.language_detector.detect(text)\n",
                "        \n",
                "        # Route to appropriate model or return not supported\n",
                "        if lang_code == 'en':\n",
                "            result = self._predict_english(text)\n",
                "            model_used = 'English (BERT)'\n",
                "        elif lang_code == 'hi':\n",
                "            result = self._predict_hindi(text)\n",
                "            model_used = 'Hindi (XLM-RoBERTa)'\n",
                "        else:\n",
                "            return {\n",
                "                'text': text,\n",
                "                'detected_language': lang_name,\n",
                "                'prediction': 'NOT SUPPORTED',\n",
                "                'confidence': 0.0,\n",
                "                'probabilities': {},\n",
                "                'model_used': None,\n",
                "                'message': f'Language \"{lang_name}\" is not supported. Only Hindi and English are supported.'\n",
                "            }\n",
                "        \n",
                "        return {\n",
                "            'text': text,\n",
                "            'detected_language': lang_name,\n",
                "            'prediction': result['prediction'],\n",
                "            'confidence': result['confidence'],\n",
                "            'probabilities': result['probabilities'],\n",
                "            'model_used': model_used\n",
                "        }\n",
                "    \n",
                "    def classify_batch(self, texts):\n",
                "        \"\"\"Classify multiple texts at once.\"\"\"\n",
                "        return [self.classify(text) for text in texts]\n",
                "    \n",
                "    def analyze(self, texts, verbose=True):\n",
                "        \"\"\"\n",
                "        Analyze multiple texts with formatted output.\n",
                "        \"\"\"\n",
                "        results = []\n",
                "        \n",
                "        if verbose:\n",
                "            print(\"\\n\" + \"=\" * 70)\n",
                "            print(\"üìä ANALYSIS RESULTS\")\n",
                "            print(\"=\" * 70)\n",
                "        \n",
                "        for i, text in enumerate(texts, 1):\n",
                "            result = self.classify(text)\n",
                "            results.append(result)\n",
                "            \n",
                "            if verbose:\n",
                "                print(f\"\\n{i}. Text: {text[:50]}...\" if len(text) > 50 else f\"\\n{i}. Text: {text}\")\n",
                "                print(f\"   üåê Language: {result['detected_language'].upper()}\")\n",
                "                \n",
                "                if result['prediction'] == 'NOT SUPPORTED':\n",
                "                    print(f\"   ‚ö†Ô∏è  {result['message']}\")\n",
                "                else:\n",
                "                    pred_emoji = {'hate': 'üî¥', 'normal': 'üü¢', 'offensive': 'üü°', 'hatespeech': 'üî¥', 'normal': 'üü¢', 'offensive': 'üü°'}\n",
                "                    emoji = pred_emoji.get(result['prediction'].lower(), '‚ö™')\n",
                "                    print(f\"   {emoji} Prediction: {result['prediction'].upper()}\")\n",
                "                    print(f\"   üìà Confidence: {result['confidence']:.1%}\")\n",
                "                    print(f\"   ü§ñ Model: {result['model_used']}\")\n",
                "        \n",
                "        if verbose:\n",
                "            print(\"\\n\" + \"=\" * 70)\n",
                "        \n",
                "        return results\n",
                "\n",
                "print(\"‚úÖ UnifiedHateSpeechClassifier class defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Initialize the Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Initializing Unified Hate Speech Classifier...\n",
                        "============================================================\n",
                        "üì± Device: cpu\n",
                        "‚úÖ Language Detector initialized\n",
                        "\n",
                        "üìö Loading English model (BERT)...\n",
                        "   ‚ö†Ô∏è  Local weights not found at './model'\n",
                        "   üì• Downloading from HuggingFace: Hate-speech-CNERG/bert-base-uncased-hatexplain\n",
                        "   Classes: ['hate speech', 'normal', 'offensive']\n",
                        "\n",
                        "üìö Loading Hindi model (XLM-RoBERTa)...\n",
                        "   Classes: ['hate', 'normal', 'offensive']\n",
                        "\n",
                        "============================================================\n",
                        "‚úÖ Unified Classifier Ready!\n",
                        "   Supported Languages: Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä), English\n",
                        "============================================================\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Initialize the unified classifier\n",
                "# If local English model weights are missing, it will automatically download from HuggingFace\n",
                "classifier = UnifiedHateSpeechClassifier(\n",
                "    english_model_path=\"./model\",\n",
                "    hindi_model_path=\"./hindi_text_classifier\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Test Cases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "======================================================================\n",
                        "üìä ANALYSIS RESULTS\n",
                        "======================================================================\n",
                        "\n",
                        "1. Text: I love spending time with my family and friends\n",
                        "   üåê Language: ENGLISH\n",
                        "   üü¢ Prediction: NORMAL\n",
                        "   üìà Confidence: 76.2%\n",
                        "   ü§ñ Model: English (BERT)\n",
                        "\n",
                        "2. Text: You are such a stupid idiot, go away\n",
                        "   üåê Language: ENGLISH\n",
                        "   üü¢ Prediction: NORMAL\n",
                        "   üìà Confidence: 67.4%\n",
                        "   ü§ñ Model: English (BERT)\n",
                        "\n",
                        "3. Text: All immigrants should be deported immediately\n",
                        "   üåê Language: ENGLISH\n",
                        "   üü¢ Prediction: NORMAL\n",
                        "   üìà Confidence: 51.9%\n",
                        "   ü§ñ Model: English (BERT)\n",
                        "\n",
                        "4. Text: ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§®‡§∞‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Æ‡•ã‡§¶‡•Ä ‡§π‡•à\n",
                        "   üåê Language: HINDI\n",
                        "   üü¢ Prediction: NORMAL\n",
                        "   üìà Confidence: 98.7%\n",
                        "   ü§ñ Model: Hindi (XLM-RoBERTa)\n",
                        "\n",
                        "5. Text: ‡§Ø‡•á ‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§ï‡•Ä ‡§î‡§≤‡§æ‡§¶ ‡§π‡•à ‡§∏‡§¨\n",
                        "   üåê Language: HINDI\n",
                        "   üî¥ Prediction: HATE\n",
                        "   üìà Confidence: 84.0%\n",
                        "   ü§ñ Model: Hindi (XLM-RoBERTa)\n",
                        "\n",
                        "6. Text: ‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§∏‡•Å‡§π‡§æ‡§µ‡§®‡§æ ‡§π‡•à\n",
                        "   üåê Language: HINDI\n",
                        "   üü¢ Prediction: NORMAL\n",
                        "   üìà Confidence: 98.3%\n",
                        "   ü§ñ Model: Hindi (XLM-RoBERTa)\n",
                        "\n",
                        "7. Text: Ye bahut bakwaas hai ‡§Ø‡§æ‡§∞\n",
                        "   üåê Language: NOT_SUPPORTED (HINGLISH/MIXED)\n",
                        "   ‚ö†Ô∏è  Language \"not_supported (hinglish/mixed)\" is not supported. Only Hindi and English are supported.\n",
                        "\n",
                        "8. Text: Tu pagal ‡§π‡•à ‡§ï‡•ç‡§Ø‡§æ bro\n",
                        "   üåê Language: NOT_SUPPORTED (HINGLISH/MIXED)\n",
                        "   ‚ö†Ô∏è  Language \"not_supported (hinglish/mixed)\" is not supported. Only Hindi and English are supported.\n",
                        "\n",
                        "9. Text: Je t'aime beaucoup mon ami\n",
                        "   üåê Language: NOT_SUPPORTED (FR)\n",
                        "   ‚ö†Ô∏è  Language \"not_supported (fr)\" is not supported. Only Hindi and English are supported.\n",
                        "\n",
                        "10. Text: Ich liebe dich sehr\n",
                        "   üåê Language: NOT_SUPPORTED (DE)\n",
                        "   ‚ö†Ô∏è  Language \"not_supported (de)\" is not supported. Only Hindi and English are supported.\n",
                        "\n",
                        "11. Text: „Åì„Çì„Å´„Å°„ÅØ\n",
                        "   üåê Language: NOT_SUPPORTED (JA)\n",
                        "   ‚ö†Ô∏è  Language \"not_supported (ja)\" is not supported. Only Hindi and English are supported.\n",
                        "\n",
                        "======================================================================\n"
                    ]
                }
            ],
            "source": [
                "# Test cases covering different scenarios\n",
                "test_texts = [\n",
                "    # ===== ENGLISH TEXTS =====\n",
                "    \"I love spending time with my family and friends\",\n",
                "    \"You are such a stupid idiot, go away\",\n",
                "    \"All immigrants should be deported immediately\",\n",
                "    \n",
                "    # ===== HINDI TEXTS =====\n",
                "    \"‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§®‡§∞‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Æ‡•ã‡§¶‡•Ä ‡§π‡•à\",\n",
                "    \"‡§Ø‡•á ‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§ï‡•Ä ‡§î‡§≤‡§æ‡§¶ ‡§π‡•à ‡§∏‡§¨\",\n",
                "    \"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§∏‡•Å‡§π‡§æ‡§µ‡§®‡§æ ‡§π‡•à\",\n",
                "    \n",
                "    # ===== HINGLISH (NOT SUPPORTED) =====\n",
                "    \"Ye bahut bakwaas hai ‡§Ø‡§æ‡§∞\",\n",
                "    \"Tu pagal ‡§π‡•à ‡§ï‡•ç‡§Ø‡§æ bro\",\n",
                "    \n",
                "    # ===== OTHER LANGUAGES (NOT SUPPORTED) =====\n",
                "    \"Je t'aime beaucoup mon ami\",  # French\n",
                "    \"Ich liebe dich sehr\",          # German\n",
                "    \"„Åì„Çì„Å´„Å°„ÅØ\",                    # Japanese\n",
                "]\n",
                "\n",
                "# Analyze all test texts\n",
                "results = classifier.analyze(test_texts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù Single Text Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Text: Enter your text here to classify\n",
                        "Language: english\n",
                        "Prediction: normal\n",
                        "Confidence: 78.0%\n",
                        "Model Used: English (BERT)\n"
                    ]
                }
            ],
            "source": [
                "# Classify a single text\n",
                "text = \"Enter your text here to classify\"\n",
                "result = classifier.classify(text)\n",
                "\n",
                "print(f\"Text: {result['text']}\")\n",
                "print(f\"Language: {result['detected_language']}\")\n",
                "print(f\"Prediction: {result['prediction']}\")\n",
                "print(f\"Confidence: {result['confidence']:.1%}\")\n",
                "if result.get('model_used'):\n",
                "    print(f\"Model Used: {result['model_used']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Interactive Testing\n",
                "\n",
                "Run the cell below to test with your own inputs:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "======================================================================\n",
                        "üìä ANALYSIS RESULTS\n",
                        "======================================================================\n",
                        "\n",
                        "1. Text: Your English text here\n",
                        "   üåê Language: ENGLISH\n",
                        "   üü¢ Prediction: NORMAL\n",
                        "   üìà Confidence: 74.7%\n",
                        "   ü§ñ Model: English (BERT)\n",
                        "\n",
                        "2. Text: ‡§Ü‡§™‡§ï‡§æ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§Ø‡§π‡§æ‡§Å\n",
                        "   üåê Language: HINDI\n",
                        "   üü¢ Prediction: NORMAL\n",
                        "   üìà Confidence: 97.9%\n",
                        "   ü§ñ Model: Hindi (XLM-RoBERTa)\n",
                        "\n",
                        "======================================================================\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[{'text': 'Your English text here',\n",
                            "  'detected_language': 'english',\n",
                            "  'prediction': 'normal',\n",
                            "  'confidence': 0.7474,\n",
                            "  'probabilities': {'hate speech': 0.0513,\n",
                            "   'normal': 0.7474,\n",
                            "   'offensive': 0.2013},\n",
                            "  'model_used': 'English (BERT)'},\n",
                            " {'text': '‡§Ü‡§™‡§ï‡§æ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§Ø‡§π‡§æ‡§Å',\n",
                            "  'detected_language': 'hindi',\n",
                            "  'prediction': 'normal',\n",
                            "  'confidence': 0.9792,\n",
                            "  'probabilities': {'hate': 0.0142, 'normal': 0.9792, 'offensive': 0.0065},\n",
                            "  'model_used': 'Hindi (XLM-RoBERTa)'}]"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Add your own test texts here\n",
                "my_texts = [\n",
                "    \"Your English text here\",\n",
                "    \"‡§Ü‡§™‡§ï‡§æ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§Ø‡§π‡§æ‡§Å\",\n",
                "]\n",
                "\n",
                "classifier.analyze(my_texts)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "data-mining",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
